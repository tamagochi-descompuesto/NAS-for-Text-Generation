{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Text generation model"]},{"cell_type":"markdown","metadata":{},"source":["This model was built following the tutorial: [Building a text generation model from scratch](https://wingedsheep.com/building-a-language-model/)."]},{"cell_type":"markdown","metadata":{},"source":["## Data preparation"]},{"cell_type":"markdown","metadata":{},"source":["First, data is read from a dataset, preprocessing the data is also done."]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[{"data":{"text/plain":["['The sailors rode the breeze clear of the rocks.',\n"," 'The weights made the rope stretch over the pulley.',\n"," 'The mechanical doll wriggled itself loose.',\n"," 'If you had eaten more, you would want less.',\n"," 'As you eat the most, you want the least.',\n"," 'The more you would want, the less you would eat.',\n"," 'I demand that the more John eat, the more he pays.',\n"," 'Mary listens to the Grateful Dead, she gets depressed.',\n"," 'The angrier Mary got, the more she looked at pictures.',\n"," 'The higher the stakes, the lower his expectations are.']"]},"execution_count":225,"metadata":{},"output_type":"execute_result"}],"source":["import csv\n","\n","with open('datasets/in_domain_dev.tsv') as file:\n","    tsv_file = csv.reader(file, delimiter='\\t')\n","\n","    data = []\n","\n","    for line in tsv_file:\n","        #Reading the data\n","        data.append(line[-1])\n","\n","data[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["Special character removal is done for better processing."]},{"cell_type":"code","execution_count":226,"metadata":{},"outputs":[{"data":{"text/plain":["['the',\n"," 'sailors',\n"," 'rode',\n"," 'the',\n"," 'breeze',\n"," 'clear',\n"," 'of',\n"," 'the',\n"," 'rocks',\n"," 'the']"]},"execution_count":226,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","from nltk.tokenize import word_tokenize\n","\n","#List all word tokens removing special characters (didn't --> didnt)\n","word_tokens = word_tokenize(\" \".join([re.sub(r'[^a-zA-Z0-9 \\n\\.]', '', line) for line in data]))\n","\n","#New token list without stopwords and punctuation symbols\n","filtered_tokens = [word.lower() for word in word_tokens if word not in ['.', ',', '?', '!']]\n","\n","filtered_tokens[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["Text tokenization:"]},{"cell_type":"code","execution_count":227,"metadata":{},"outputs":[{"data":{"text/plain":["'{\"the\": 1, \"to\": 2, \"a\": 3, \"that\": 4, \"i\": 5, \"is\": 6, \"john\": 7, \"and\": 8, \"of\": 9, \"in\": 10, \"you\": 11, \"it\": 12, \"was\": 13, \"mary\": 14, \"he\": 15, \"did\": 16, \"which\": 17, \"will\": 18, \"be\": 19, \"book\": 20, \"bill\": 21, \"we\": 22, \"who\": 23, \"on\": 24, \"with\": 25, \"for\": 26, \"this\": 27, \"more\": 28, \"at\": 29, \"my\": 30, \"they\": 31, \"her\": 32, \"about\": 33, \"some\": 34, \"as\": 35, \"there\": 36, \"himself\": 37, \"any\": 38, \"had\": 39, \"would\": 40, \"eat\": 41, \"his\": 42, \"not\": 43, \"have\": 44, \"man\": 45, \"him\": 46, \"can\": 47, \"by\": 48, \"read\": 49, \"she\": 50, \"has\": 51, \"up\": 52, \"me\": 53, \"put\": 54, \"want\": 55, \"leave\": 56, \"from\": 57, \"what\": 58, \"gave\": 59, \"saw\": 60, \"anson\": 61, \"if\": 62, \"said\": 63, \"like\": 64, \"but\": 65, \"one\": 66, \"were\": 67, \"kim\": 68, \"got\": 69, \"should\": 70, \"could\": 71, \"been\": 72, \"place\": 73, \"didnt\": 74, \"know\": 75, \"do\": 76, \"before\": 77, \"no\": 78, \"picture\": 79, \"made\": 80, \"fred\": 81, \"get\": 82, \"likes\": 83, \"never\": 84, \"box\": 85, \"ball\": 86, \"out\": 87, \"talked\": 88, \"an\": 89, \"left\": 90, \"tom\": 91, \"these\": 92, \"girl\": 93, \"into\": 94, \"bought\": 95, \"party\": 96, \"red\": 97, \"most\": 98, \"are\": 99, \"than\": 100, \"does\": 101, \"hates\": 102, \"sally\": 103, \"because\": 104, \"tried\": 105, \"water\": 106, \"even\": 107, \"love\": 108, \"buy\": 109, \"where\": 110, \"ate\": 111, \"yesterday\": 112, \"somebody\": 113, \"ask\": 114, \"sleep\": 115, \"may\": 116, \"mother\": 117, \"please\": 118, \"anything\": 119, \"sent\": 120, \"park\": 121, \"job\": 122, \"car\": 123, \"desk\": 124, \"so\": 125, \"tomorrow\": 126, \"your\": 127, \"medea\": 128, \"itself\": 129, \"less\": 130, \"pictures\": 131, \"people\": 132, \"sick\": 133, \"every\": 134, \"seems\": 135, \"believe\": 136, \"bob\": 137, \"kissed\": 138, \"good\": 139, \"violin\": 140, \"go\": 141, \"wont\": 142, \"wants\": 143, \"wanted\": 144, \"came\": 145, \"work\": 146, \"im\": 147, \"after\": 148, \"easily\": 149, \"always\": 150, \"smith\": 151, \"how\": 152, \"peter\": 153, \"money\": 154, \"sam\": 155, \"might\": 156, \"tall\": 157, \"easy\": 158, \"rain\": 159, \"student\": 160, \"kicked\": 161, \"believed\": 162, \"books\": 163, \"table\": 164, \"flyer\": 165, \"children\": 166, \"many\": 167, \"cake\": 168, \"students\": 169, \"very\": 170, \"everyone\": 171, \"english\": 172, \"them\": 173, \"found\": 174, \"without\": 175, \"each\": 176, \"happy\": 177, \"coat\": 178, \"thinks\": 179, \"jean\": 180, \"give\": 181, \"susan\": 182, \"working\": 183, \"seen\": 184, \"boy\": 185, \"tree\": 186, \"persuaded\": 187, \"sue\": 188, \"men\": 189, \"us\": 190, \"having\": 191, \"too\": 192, \"sure\": 193, \"talk\": 194, \"meets\": 195, \"cats\": 196, \"short\": 197, \"fire\": 198, \"against\": 199, \"hard\": 200, \"difficult\": 201, \"washed\": 202, \"boys\": 203, \"live\": 204, \"route\": 205, \"150\": 206, \"crosses\": 207, \"river\": 208, \"mike\": 209, \"cut\": 210, \"everybody\": 211, \"knows\": 212, \"blond\": 213, \"papers\": 214, \"play\": 215, \"around\": 216, \"package\": 217, \"offered\": 218, \"old\": 219, \"farmer\": 220, \"loaded\": 221, \"terry\": 222, \"really\": 223, \"onto\": 224, \"jessica\": 225, \"boxes\": 226, \"under\": 227, \"carla\": 228, \"new\": 229, \"presented\": 230, \"linda\": 231, \"whispered\": 232, \"ellen\": 233, \"well\": 234, \"last\": 235, \"friend\": 236, \"when\": 237, \"computer\": 238, \"dog\": 239, \"country\": 240, \"think\": 241, \"other\": 242, \"loves\": 243, \"black\": 244, \"asked\": 245, \"thought\": 246, \"heidi\": 247, \"flying\": 248, \"harriet\": 249, \"churchill\": 250, \"six\": 251, \"demonized\": 252, \"myself\": 253, \"over\": 254, \"eaten\": 255, \"looked\": 256, \"obnoxious\": 257, \"beer\": 258, \"appear\": 259, \"news\": 260, \"talks\": 261, \"lot\": 262, \"eager\": 263, \"gas\": 264, \"bubbled\": 265, \"leaked\": 266, \"bottle\": 267, \"fill\": 268, \"free\": 269, \"lay\": 270, \"probably\": 271, \"though\": 272, \"actually\": 273, \"find\": 274, \"plays\": 275, \"all\": 276, \"went\": 277, \"abroad\": 278, \"remembered\": 279, \"cant\": 280, \"freds\": 281, \"story\": 282, \"also\": 283, \"eating\": 284, \"bother\": 285, \"class\": 286, \"mag\": 287, \"hadnt\": 288, \"report\": 289, \"fruit\": 290, \"cookies\": 291, \"rusty\": 292, \"only\": 293, \"something\": 294, \"fort\": 295, \"station\": 296, \"proof\": 297, \"claim\": 298, \"given\": 299, \"factory\": 300, \"build\": 301, \"secret\": 302, \"drives\": 303, \"henry\": 304, \"grapes\": 305, \"president\": 306, \"dad\": 307, \"lives\": 308, \"hudson\": 309, \"make\": 310, \"teacher\": 311, \"hat\": 312, \"quip\": 313, \"wore\": 314, \"must\": 315, \"fuzz\": 316, \"ill\": 317, \"doctor\": 318, \"definitely\": 319, \"try\": 320, \"socks\": 321, \"sonatas\": 322, \"father\": 323, \"here\": 324, \"name\": 325, \"writer\": 326, \"playwright\": 327, \"vienna\": 328, \"bench\": 329, \"world\": 330, \"basket\": 331, \"sarah\": 332, \"promised\": 333, \"cart\": 334, \"apples\": 335, \"janet\": 336, \"broke\": 337, \"pulled\": 338, \"movie\": 339, \"shocks\": 340, \"sharon\": 341, \"acorn\": 342, \"grow\": 343, \"oak\": 344, \"sugar\": 345, \"threw\": 346, \"round\": 347, \"wagon\": 348, \"nora\": 349, \"slid\": 350, \"jones\": 351, \"carmen\": 352, \"taped\": 353, \"wall\": 354, \"child\": 355, \"differ\": 356, \"cynthia\": 357, \"laughed\": 358, \"heart\": 359, \"poisoned\": 360, \"through\": 361, \"wonderful\": 362, \"opportunity\": 363, \"policeman\": 364, \"met\": 365, \"young\": 366, \"meet\": 367, \"proud\": 368, \"their\": 369, \"dont\": 370, \"sandy\": 371, \"afraid\": 372, \"seem\": 373, \"look\": 374, \"committee\": 375, \"study\": 376, \"reading\": 377, \"while\": 378, \"drank\": 379, \"golden\": 380, \"hair\": 381, \"tries\": 382, \"bed\": 383, \"kick\": 384, \"ann\": 385, \"spent\": 386, \"wrote\": 387, \"struck\": 388, \"whose\": 389, \"whom\": 390, \"person\": 391, \"sit\": 392, \"topic\": 393, \"invaded\": 394, \"fido\": 395, \"owl\": 396, \"doesnt\": 397, \"mice\": 398, \"tickets\": 399, \"its\": 400, \"those\": 401, \"much\": 402, \"believes\": 403, \"dave\": 404, \"herself\": 405, \"hit\": 406, \"knew\": 407, \"ive\": 408, \"lost\": 409, \"wallet\": 410, \"or\": 411, \"takes\": 412, \"long\": 413, \"time\": 414, \"johns\": 415, \"andy\": 416, \"salmon\": 417, \"flavored\": 418, \"candy\": 419, \"bars\": 420, \"danced\": 421, \"am\": 422, \"rich\": 423, \"id\": 424, \"diamond\": 425, \"ring\": 426, \"collection\": 427, \"dance\": 428, \"pilot\": 429, \"plane\": 430, \"attended\": 431, \"weeks\": 432, \"huge\": 433, \"rally\": 434, \"signed\": 435, \"petition\": 436, \"tiger\": 437, \"orange\": 438, \"fur\": 439, \"marked\": 440, \"stripes\": 441, \"pick\": 442, \"flower\": 443, \"building\": 444, \"took\": 445, \"emma\": 446, \"oclock\": 447, \"whiskey\": 448, \"became\": 449, \"nine\": 450, \"arrived\": 451, \"come\": 452, \"fought\": 453, \"nurse\": 454, \"hopes\": 455, \"going\": 456, \"jason\": 457, \"poison\": 458, \"banana\": 459, \"monkey\": 460, \"sailors\": 461, \"rode\": 462, \"breeze\": 463, \"clear\": 464, \"rocks\": 465, \"weights\": 466, \"rope\": 467, \"stretch\": 468, \"pulley\": 469, \"mechanical\": 470, \"doll\": 471, \"wriggled\": 472, \"loose\": 473, \"least\": 474, \"demand\": 475, \"pays\": 476, \"listens\": 477, \"grateful\": 478, \"dead\": 479, \"gets\": 480, \"depressed\": 481, \"angrier\": 482, \"higher\": 483, \"stakes\": 484, \"lower\": 485, \"expectations\": 486, \"attention\": 487, \"pay\": 488, \"lots\": 489, \"smoke\": 490, \"embarrassed\": 491, \"becomes\": 492, \"senator\": 493, \"become\": 494, \"corrupt\": 495, \"lobbyists\": 496, \"visit\": 497, \"marianne\": 498, \"life\": 499, \"bangor\": 500, \"mickey\": 501, \"tended\": 502, \"discussion\": 503, \"win\": 504, \"contained\": 505, \"tube\": 506, \"escaped\": 507, \"kettle\": 508, \"tub\": 509, \"whole\": 510, \"tank\": 511, \"fluid\": 512, \"owns\": 513, \"stay\": 514, \"consider\": 515, \"courts\": 516, \"klaus\": 517, \"guilty\": 518, \"murder\": 519, \"beautifully\": 520, \"clearly\": 521, \"immediately\": 522, \"learn\": 523, \"french\": 524, \"perfectly\": 525, \"home\": 526, \"represented\": 527, \"seriously\": 528, \"dean\": 529, \"genuine\": 530, \"linguist\": 531, \"nice\": 532, \"intended\": 533, \"either\": 534, \"invite\": 535, \"someone\": 536, \"couldnt\": 537, \"decide\": 538, \"joe\": 539, \"hollys\": 540, \"claimed\": 541, \"cabbage\": 542, \"holly\": 543, \"shouldnt\": 544, \"introduced\": 545, \"bartender\": 546, \"joes\": 547, \"neuroses\": 548, \"patrons\": 549, \"jos\": 550, \"lilly\": 551, \"reviewed\": 552, \"everything\": 553, \"often\": 554, \"problem\": 555, \"perceives\": 556, \"hundred\": 557, \"surrounded\": 558, \"elected\": 559, \"incompetent\": 560, \"submit\": 561, \"preferred\": 562, \"lemons\": 563, \"limes\": 564, \"let\": 565, \"whining\": 566, \"walking\": 567, \"toward\": 568, \"railroad\": 569, \"lied\": 570, \"attributed\": 571, \"circuit\": 572, \"caused\": 573, \"overloaded\": 574, \"transducer\": 575, \"destroyed\": 576, \"mayor\": 577, \"regarded\": 578, \"being\": 579, \"absurd\": 580, \"proposal\": 581, \"sidewalk\": 582, \"dartmouth\": 583, \"remain\": 584, \"drowning\": 585, \"law\": 586, \"rescue\": 587, \"muriel\": 588, \"nothing\": 589, \"else\": 590, \"insulted\": 591, \"understood\": 592, \"rutherford\": 593, \"feel\": 594, \"arch\": 595, \"show\": 596, \"set\": 597, \"recursive\": 598, \"madrigals\": 599, \"lute\": 600, \"sings\": 601, \"sound\": 602, \"lousy\": 603, \"picked\": 604, \"turnips\": 605, \"suzie\": 606, \"prepare\": 607, \"elect\": 608, \"guardians\": 609, \"employer\": 610, \"sane\": 611, \"trying\": 612, \"now\": 613, \"wind\": 614, \"gotten\": 615, \"plot\": 616, \"negotiate\": 617, \"honorable\": 618, \"end\": 619, \"war\": 620, \"vietnam\": 621, \"politics\": 622, \"friends\": 623, \"expected\": 624, \"reporters\": 625, \"principal\": 626, \"donated\": 627, \"wire\": 628, \"convicts\": 629, \"cages\": 630, \"looking\": 631, \"force\": 632, \"sometimes\": 633, \"pleased\": 634, \"winston\": 635, \"worried\": 636, \"worry\": 637, \"heres\": 638, \"knife\": 639, \"onions\": 640, \"fluffy\": 641, \"maxwell\": 642, \"quite\": 643, \"younger\": 644, \"woman\": 645, \"older\": 646, \"sheila\": 647, \"finish\": 648, \"grading\": 649, \"ready\": 650, \"planned\": 651, \"poor\": 652, \"started\": 653, \"umbrella\": 654, \"cops\": 655, \"spoke\": 656, \"janitor\": 657, \"terrible\": 658, \"robbery\": 659, \"wears\": 660, \"swinger\": 661, \"binoculars\": 662, \"allege\": 663, \"roger\": 664, \"nobody\": 665, \"delicatessen\": 666, \"ever\": 667, \"buys\": 668, \"credit\": 669, \"remember\": 670, \"misgivings\": 671, \"nor\": 672, \"marry\": 673, \"certain\": 674, \"felicia\": 675, \"off\": 676, \"halfway\": 677, \"square\": 678, \"festive\": 679, \"air\": 680, \"worker\": 681, \"forgive\": 682, \"comment\": 683, \"launched\": 684, \"rocket\": 685, \"moon\": 686, \"blew\": 687, \"catherine\": 688, \"then\": 689, \"son\": 690, \"instead\": 691, \"lent\": 692, \"partway\": 693, \"tony\": 694, \"dumped\": 695, \"martha\": 696, \"carved\": 697, \"baby\": 698, \"toy\": 699, \"wood\": 700, \"bread\": 701, \"cuts\": 702, \"finger\": 703, \"cup\": 704, \"visitor\": 705, \"rang\": 706, \"bell\": 707, \"room\": 708, \"turned\": 709, \"frog\": 710, \"mixed\": 711, \"butter\": 712, \"brian\": 713, \"fence\": 714, \"stick\": 715, \"mira\": 716, \"condemned\": 717, \"accident\": 718, \"investigated\": 719, \"area\": 720, \"bombs\": 721, \"sensed\": 722, \"eagerness\": 723, \"praised\": 724, \"dedication\": 725, \"volunteers\": 726, \"earth\": 727, \"smiled\": 728, \"charming\": 729, \"smile\": 730, \"sandra\": 731, \"beamed\": 732, \"cheerful\": 733, \"welcome\": 734, \"youve\": 735, \"lived\": 736, \"paperback\": 737, \"lift\": 738, \"lifted\": 739, \"crammed\": 740, \"truck\": 741, \"lora\": 742, \"buttered\": 743, \"toast\": 744, \"shoveled\": 745, \"walk\": 746, \"amanda\": 747, \"carried\": 748, \"pamela\": 749, \"packages\": 750, \"drive\": 751, \"york\": 752, \"chair\": 753, \"pushed\": 754, \"brown\": 755, \"plaque\": 756, \"dress\": 757, \"obtained\": 758, \"spare\": 759, \"part\": 760, \"hardware\": 761, \"store\": 762, \"michelle\": 763, \"kept\": 764, \"frances\": 765, \"hid\": 766, \"presents\": 767, \"drawer\": 768, \"needle\": 769, \"poked\": 770, \"cloth\": 771, \"carrie\": 772, \"touched\": 773, \"cat\": 774, \"herman\": 775, \"whipped\": 776, \"cream\": 777, \"clung\": 778, \"together\": 779, \"apart\": 780, \"jeweller\": 781, \"scribbled\": 782, \"contract\": 783, \"gardener\": 784, \"grew\": 785, \"shaped\": 786, \"loaf\": 787, \"amused\": 788, \"rachel\": 789, \"melons\": 790, \"selling\": 791, \"present\": 792, \"conditions\": 793, \"warned\": 794, \"helen\": 795, \"skating\": 796, \"thin\": 797, \"ice\": 798, \"nibbled\": 799, \"carrot\": 800, \"chewed\": 801, \"paul\": 802, \"winked\": 803, \"lip\": 804, \"pounding\": 805, \"fainted\": 806, \"hunger\": 807, \"witch\": 808, \"grandfather\": 809, \"clock\": 810, \"ticked\": 811, \"hallway\": 812, \"squeaked\": 813, \"door\": 814, \"fragrant\": 815, \"stew\": 816, \"soaring\": 817, \"temperatures\": 818, \"predicted\": 819, \"weekend\": 820, \"fluttered\": 821, \"flags\": 822, \"voices\": 823, \"echoed\": 824, \"hall\": 825, \"stream\": 826, \"twists\": 827, \"valley\": 828, \"jumped\": 829, \"little\": 830, \"white\": 831, \"rabbit\": 832, \"penny\": 833, \"skated\": 834, \"rink\": 835, \"jackie\": 836, \"accompanied\": 837, \"rose\": 838, \"information\": 839, \"provided\": 840, \"offers\": 841, \"advice\": 842, \"delicious\": 843, \"recommend\": 844, \"pastry\": 845, \"explode\": 846, \"wash\": 847, \"jobs\": 848, \"jeopardy\": 849, \"several\": 850, \"night\": 851, \"monkeys\": 852, \"leader\": 853, \"sounded\": 854, \"agree\": 855, \"called\": 856, \"fool\": 857, \"forgot\": 858, \"tastes\": 859, \"genius\": 860, \"remained\": 861, \"noodle\": 862, \"quietly\": 863, \"hammered\": 864, \"metal\": 865, \"removed\": 866, \"ballet\": 867, \"shoes\": 868, \"week\": 869, \"benny\": 870, \"worked\": 871, \"shoe\": 872, \"hoped\": 873, \"sing\": 874, \"proved\": 875, \"decisive\": 876, \"factor\": 877, \"crocodile\": 878, \"devoured\": 879, \"doughnut\": 880, \"bathtub\": 881, \"placed\": 882, \"behind\": 883, \"garage\": 884, \"depends\": 885, \"taught\": 886, \"syntax\": 887, \"regards\": 888, \"chickens\": 889, \"fond\": 890, \"rules\": 891, \"require\": 892, \"executives\": 893, \"polite\": 894, \"unpopular\": 895, \"nominated\": 896, \"taking\": 897, \"possible\": 898, \"fta\": 899, \"paid\": 900, \"feasibility\": 901, \"setting\": 902, \"national\": 903, \"network\": 904, \"rude\": 905, \"pamphlet\": 906, \"judy\": 907, \"garbage\": 908, \"studying\": 909, \"reads\": 910, \"conrads\": 911, \"darkness\": 912, \"university\": 913, \"rotten\": 914, \"neither\": 915, \"failed\": 916, \"doorway\": 917, \"waved\": 918, \"ferocious\": 919, \"bite\": 920, \"yourself\": 921, \"seemed\": 922, \"intelligent\": 923, \"stephen\": 924, \"fountain\": 925, \"fun\": 926, \"hide\": 927, \"sheep\": 928, \"hope\": 929, \"france\": 930, \"expect\": 931, \"office\": 932, \"school\": 933, \"town\": 934, \"tour\": 935, \"art\": 936, \"galleries\": 937, \"neednt\": 938, \"take\": 939, \"exam\": 940, \"spend\": 941, \"vacation\": 942, \"italy\": 943, \"golf\": 944, \"george\": 945, \"hasnt\": 946, \"statesman\": 947, \"scarcely\": 948, \"worth\": 949, \"mentioning\": 950, \"unicorns\": 951, \"koreas\": 952, \"famous\": 953, \"poets\": 954, \"lines\": 955, \"apparently\": 956, \"unidentified\": 957, \"victim\": 958, \"during\": 959, \"early\": 960, \"morning\": 961, \"hours\": 962, \"driven\": 963, \"ricky\": 964, \"relied\": 965, \"slept\": 966, \"pound\": 967, \"weighed\": 968, \"five\": 969, \"thousand\": 970, \"dollars\": 971, \"fed\": 972, \"politician\": 973, \"vote\": 974, \"guess\": 975, \"fixed\": 976, \"efforts\": 977, \"achieve\": 978, \"peace\": 979, \"honor\": 980, \"house\": 981, \"baker\": 982, \"bagels\": 983, \"relax\": 984, \"jack\": 985, \"jenny\": 986, \"fell\": 987, \"grows\": 988, \"peaches\": 989, \"choose\": 990, \"getting\": 991, \"approval\": 992, \"bored\": 993, \"reason\": 994, \"why\": 995, \"resigned\": 996, \"bothers\": 997, \"coughs\": 998, \"iraq\": 999, \"bites\": 1000, \"annoys\": 1001, \"barks\": 1002, \"achieved\": 1003, \"best\": 1004, \"result\": 1005, \"angela\": 1006, \"peasant\": 1007, \"kind\": 1008, \"anyone\": 1009, \"hunt\": 1010, \"candidate\": 1011, \"interest\": 1012, \"semantics\": 1013, \"admitted\": 1014, \"department\": 1015, \"author\": 1016, \"contribution\": 1017, \"written\": 1018, \"language\": 1019, \"provide\": 1020, \"summary\": 1021, \"sympathy\": 1022, \"urban\": 1023, \"guerillas\": 1024, \"helped\": 1025, \"isnt\": 1026, \"bad\": 1027, \"angry\": 1028, \"hungry\": 1029, \"whined\": 1030, \"shes\": 1031, \"pleasant\": 1032, \"hate\": 1033, \"criticize\": 1034, \"carter\": 1035, \"matter\": 1036, \"manuscript\": 1037, \"type\": 1038, \"hes\": 1039, \"reliable\": 1040, \"trouble\": 1041, \"high\": 1042, \"bamboo\": 1043, \"errors\": 1044, \"20\": 1045, \"drew\": 1046, \"rosie\": 1047, \"magazine\": 1048, \"ads\": 1049, \"dan\": 1050, \"erin\": 1051, \"jaime\": 1052, \"alina\": 1053, \"hopefully\": 1054, \"winter\": 1055, \"snow\": 1056, \"blue\": 1057, \"leather\": 1058, \"shows\": 1059, \"betsy\": 1060, \"pretty\": 1061, \"gwen\": 1062, \"baseball\": 1063, \"answer\": 1064, \"mind\": 1065, \"brand\": 1066, \"crumb\": 1067, \"chased\": 1068, \"cent\": 1069, \"poems\": 1070, \"blackwell\": 1071, \"cover\": 1072, \"fear\": 1073, \"dogs\": 1074, \"buildings\": 1075, \"roof\": 1076, \"leaking\": 1077, \"panthers\": 1078, \"dark\": 1079, \"colin\": 1080, \"mortgage\": 1081, \"cab\": 1082, \"robbed\": 1083, \"bank\": 1084, \"asparagus\": 1085, \"drum\": 1086, \"evil\": 1087, \"classroom\": 1088, \"phillip\": 1089, \"medal\": 1090, \"soldier\": 1091, \"calvin\": 1092, \"homework\": 1093, \"sylvia\": 1094, \"slapping\": 1095, \"jeff\": 1096, \"upside\": 1097, \"head\": 1098, \"martial\": 1099, \"arts\": 1100, \"hed\": 1101, \"likely\": 1102, \"manager\": 1103, \"sunk\": 1104, \"gorilla\": 1105, \"model\": 1106, \"airplane\": 1107, \"lucy\": 1108, \"mugged\": 1109, \"salad\": 1110, \"filled\": 1111, \"lima\": 1112, \"beans\": 1113, \"four\": 1114, \"arriving\": 1115, \"reluctant\": 1116, \"robert\": 1117, \"macarena\": 1118, \"admire\": 1119, \"hunts\": 1120, \"dinner\": 1121, \"whoever\": 1122, \"anybody\": 1123, \"albino\": 1124, \"see\": 1125, \"duty\": 1126, \"today\": 1127, \"few\": 1128, \"dodgers\": 1129, \"beat\": 1130, \"sox\": 1131, \"beaten\": 1132, \"giants\": 1133, \"computed\": 1134, \"tax\": 1135, \"kennel\": 1136, \"sleeps\": 1137, \"stolen\": 1138, \"raw\": 1139, \"eggplant\": 1140, \"wealthy\": 1141, \"piano\": 1142, \"fiance\": 1143, \"stole\": 1144, \"turkey\": 1145, \"captain\": 1146, \"wentworth\": 1147, \"letter\": 1148, \"anne\": 1149, \"elliott\": 1150, \"alison\": 1151, \"coming\": 1152, \"realised\": 1153, \"sir\": 1154, \"thomas\": 1155, \"offended\": 1156, \"fanny\": 1157, \"regretted\": 1158, \"aunt\": 1159, \"norris\": 1160, \"knowing\": 1161, \"train\": 1162, \"elses\": 1163, \"pocket\": 1164, \"magnus\": 1165, \"ireland\": 1166, \"send\": 1167, \"idea\": 1168, \"dismayed\": 1169, \"prime\": 1170, \"minister\": 1171, \"dome\": 1172, \"dull\": 1173, \"right\": 1174, \"knock\": 1175, \"admired\": 1176, \"mr\": 1177, \"knightley\": 1178, \"boring\": 1179, \"ethel\": 1180, \"wishes\": 1181, \"awkward\": 1182, \"questions\": 1183, \"food\": 1184, \"window\": 1185, \"broken\": 1186, \"hammer\": 1187, \"fugitive\": 1188, \"motionless\": 1189, \"order\": 1190, \"avoid\": 1191, \"discovery\": 1192, \"guard\": 1193, \"marched\": 1194, \"prisoners\": 1195, \"yard\": 1196, \"frank\": 1197, \"crossed\": 1198, \"street\": 1199, \"attacked\": 1200, \"fiona\": 1201, \"5\": 1202, \"both\": 1203, \"expecting\": 1204, \"opinion\": 1205, \"desirable\": 1206, \"pat\": 1207, \"awarded\": 1208, \"fleece\": 1209, \"award\": 1210, \"upset\": 1211, \"alienated\": 1212, \"beating\": 1213, \"sale\": 1214, \"article\": 1215, \"file\": 1216, \"goes\": 1217, \"buying\": 1218, \"dollar\": 1219, \"bobbie\": 1220, \"dime\": 1221, \"mailbox\": 1222, \"first\": 1223, \"edition\": 1224, \"richard\": 1225, \"iii\": 1226, \"1000\": 1227, \"deadly\": 1228, \"gentle\": 1229, \"henri\": 1230, \"cooking\": 1231, \"fix\": 1232, \"carol\": 1233, \"loaned\": 1234, \"valuable\": 1235, \"manuscripts\": 1236, \"library\": 1237, \"paper\": 1238, \"suddenly\": 1239, \"two\": 1240, \"inspectors\": 1241, \"ins\": 1242, \"soup\": 1243, \"cooks\": 1244, \"thickens\": 1245, \"charity\": 1246, \"wonders\": 1247, \"sophie\": 1248, \"theater\": 1249, \"finished\": 1250, \"lemonade\": 1251, \"marys\": 1252, \"revealed\": 1253, \"heard\": 1254, \"criticized\": 1255, \"themselves\": 1256, \"smart\": 1257, \"sad\": 1258, \"considers\": 1259, \"available\": 1260, \"giving\": 1261, \"blood\": 1262, \"sweat\": 1263, \"tears\": 1264, \"speech\": 1265, \"promise\": 1266, \"shave\": 1267, \"convinced\": 1268, \"bears\": 1269, \"sniffed\": 1270, \"steal\": 1271, \"talismans\": 1272, \"witches\": 1273, \"dangerous\": 1274, \"yourselves\": 1275, \"khyber\": 1276, \"true\": 1277, \"treat\": 1278, \"accurate\": 1279, \"description\": 1280, \"governments\": 1281, \"imposition\": 1282, \"fine\": 1283, \"athena\": 1284, \"help\": 1285, \"david\": 1286, \"day\": 1287, \"programme\": 1288, \"euripides\": 1289, \"radio\": 1290, \"4\": 1291, \"tonight\": 1292, \"denied\": 1293, \"poisoning\": 1294, \"phoenix\": 1295, \"elixir\": 1296, \"hidden\": 1297, \"hole\": 1298, \"ground\": 1299, \"extremely\": 1300, \"frantically\": 1301, \"trade\": 1302, \"felt\": 1303, \"handsome\": 1304, \"gilgamesh\": 1305, \"dragon\": 1306, \"fortunately\": 1307, \"hermione\": 1308, \"passed\": 1309, \"biology\": 1310, \"washing\": 1311, \"liked\": 1312, \"butler\": 1313, \"dinah\": 1314, \"happens\": 1315, \"linguists\": 1316, \"argue\": 1317, \"fierce\": 1318, \"battle\": 1319, \"king\": 1320, \"city\": 1321, \"disgruntled\": 1322, \"pigs\": 1323, \"ditches\": 1324, \"humans\": 1325, \"burn\": 1326, \"stinks\": 1327, \"aphrodite\": 1328, \"omnipotent\": 1329, \"agamemnon\": 1330, \"maniac\": 1331, \"wondered\": 1332, \"inquired\": 1333, \"swim\": 1334, \"bookcase\": 1335, \"ran\": 1336, \"shaved\": 1337, \"muscle\": 1338, \"bound\": 1339}'"]},"execution_count":227,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","\n","tokenizer.fit_on_texts(filtered_tokens)\n","\n","tokenizer.get_config()['word_index']"]},{"cell_type":"markdown","metadata":{},"source":["Finally, a numeric representation for all the text is built:"]},{"cell_type":"code","execution_count":228,"metadata":{},"outputs":[],"source":["def text_to_token_list(word_dic: dict, text: list) -> list:\n","    '''\n","    Function that turns a text list into a token list, example:\n","    [\"Hello\",  \"world\", \"I\",  \"am\",  \"a\",  \"sentence\"] --> [0, 1, 3, 4, 2, 5]\n","\n","    Parameters:\n","    word_dic: A dictionary that relates every word within the text with a numeric value, i.e. {\"Hello\": 0, \"world\": 1, \"a\": 2, \"I\": 3, \"am\": 4, \"sentence\": 5}.\n","    text: A list containing the text to tokenize.\n","\n","    Outputs:\n","    token_list: A list that contains every word in the text converted into its corresponding numeric representation.\n","    '''\n","    if type(text) == list:\n","        return [word_dic[word] for word in text]\n","    else:\n","        return [word_dic[word] for word in text.split()]"]},{"cell_type":"code","execution_count":229,"metadata":{},"outputs":[{"data":{"text/plain":["[1, 461, 462, 1, 463, 464, 9, 1, 465, 1]"]},"execution_count":229,"metadata":{},"output_type":"execute_result"}],"source":["#Converting the word list into a token_list\n","token_list = text_to_token_list(eval(tokenizer.get_config()['word_index']), filtered_tokens)\n","\n","token_list[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["## Building the model."]},{"cell_type":"markdown","metadata":{},"source":["### Layers of the model."]},{"cell_type":"markdown","metadata":{},"source":["Making and embedding layer:"]},{"cell_type":"code","execution_count":230,"metadata":{},"outputs":[],"source":["import torch\n","\n","class TokenEmbedding(torch.nn.Module):\n","    \"\"\"\n","    PyTorch module that converts tokens into embeddings.\n","\n","    Input dimension is: (batch_size, sequence_length)\n","    Output dimension is: (batch_size, sequence_length, d_model)\n","    \"\"\"\n","\n","    def __init__(self, d_model, number_of_tokens):\n","        '''\n","        number_of_tokens: total number of unique tokens the model can find in the input.\n","        d_model: dimensionality of the embedding vectors. \n","        '''\n","        super().__init__()\n","        self.embedding_layer = torch.nn.Embedding(\n","            num_embeddings=number_of_tokens,\n","            embedding_dim=d_model\n","        )\n","\n","    def forward(self, x):\n","        return self.embedding_layer(x)"]},{"cell_type":"markdown","metadata":{},"source":["Making the positional encoding layer:"]},{"cell_type":"code","execution_count":231,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","class PositionalEncoding(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the \n","    transformer's input embeddings to provide a sense of position of the sequence elements.\n","    \"\"\"\n","\n","    def __init__(self, d_model, max_sequence_length):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.max_sequence_length = max_sequence_length\n","        self.positional_encoding = self.create_positional_encoding()\n","\n","    def create_positional_encoding(self):\n","        \"\"\"\n","        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n","        \"\"\"\n","\n","        # Initialize positional encoding matrix\n","        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n","\n","        # Calculate positional encoding for each position and each dimension\n","        for pos in range(self.max_sequence_length):\n","            for i in range(0, self.d_model, 2):\n","                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n","                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n","                \n","                if i + 1 < self.d_model:\n","                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n","                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n","\n","        # Convert numpy array to PyTorch tensor and return it\n","        return torch.from_numpy(positional_encoding).float()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Adds the positional encoding to the input embeddings at the corresponding positions.\n","        \"\"\"\n","        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n","        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure \n","        # we're not adding extra positional encodings.\n","        return x + self.positional_encoding[:x.size(1), :]"]},{"cell_type":"markdown","metadata":{},"source":["Coding a self attention layer:"]},{"cell_type":"code","execution_count":232,"metadata":{},"outputs":[],"source":["class MaskedSelfAttention(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for a self attention layer.\n","    This layer is used in the MultiHeadedSelfAttention module.\n","\n","    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n","    Output dimension is: (batch_size, sequence_length, head_dimension)\n","    \"\"\"\n","\n","    def __init__(self, embedding_dimension, head_dimension):\n","        super().__init__()\n","        self.embedding_dimension = embedding_dimension\n","        self.head_dimension = head_dimension\n","        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n","        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n","        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n","        self.softmax = torch.nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask):\n","        \"\"\"\n","        Compute the self attention.\n","\n","        x dimension is: (batch_size, sequence_length, embedding_dimension)\n","        output dimension is: (batch_size, sequence_length, head_dimension)\n","        mask dimension is: (batch_size, sequence_length)\n","\n","        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n","        \"\"\"\n","\n","        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n","        query = self.query_layer(x)\n","        key = self.key_layer(x)\n","        value = self.value_layer(x)\n","\n","        # Calculate the attention weights.\n","        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n","        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n","\n","        # Scale the attention weights.\n","        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n","\n","        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n","        # This will make the softmax output 0 for these values.\n","        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n","        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n","\n","        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n","        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n","        attention_scores = self.softmax(attention_weights)\n","\n","        # The attention scores are multiplied by the value\n","        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n","        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n","        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n","        return torch.bmm(attention_scores, value)"]},{"cell_type":"markdown","metadata":{},"source":["A multi headed self attention layer:"]},{"cell_type":"code","execution_count":233,"metadata":{},"outputs":[],"source":["class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for a multi head attention layer.\n","\n","    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n","    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n","    \"\"\"\n","\n","    def __init__(self, embedding_dimension, number_of_heads):\n","        super().__init__()\n","        self.embedding_dimension = embedding_dimension\n","        self.head_dimension = embedding_dimension // number_of_heads\n","        self.number_of_heads = number_of_heads\n","\n","        # Create the self attention modules\n","        self.self_attentions = torch.nn.ModuleList(\n","            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n","\n","        # Create a linear layer to combine the outputs of the self attention modules\n","        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n","\n","    def forward(self, x, mask):\n","        \"\"\"\n","        Compute the multi head attention.\n","\n","        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        mask dimensions are: (batch_size, sequence_length)\n","        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n","        \"\"\"\n","        # Compute the self attention for each head\n","        # self_attention_outputs dimensions are:\n","        # (number_of_heads, batch_size, sequence_length, head_dimension)\n","        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n","\n","        # Concatenate the self attention outputs\n","        # self_attention_outputs_concatenated dimensions are:\n","        # (batch_size, sequence_length, number_of_heads * head_dimension)\n","        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n","\n","        # Apply the output layer to the concatenated self attention outputs\n","        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        return self.output_layer(concatenated_self_attention_outputs)"]},{"cell_type":"markdown","metadata":{},"source":["A feed forward neural network layer for the decoder:"]},{"cell_type":"code","execution_count":234,"metadata":{},"outputs":[],"source":["class FeedForward(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for a feed forward layer.\n","\n","    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dimension, feed_forward_dimension):\n","        super().__init__()\n","        self.embedding_dimension = embedding_dimension\n","        self.feed_forward_dimension = feed_forward_dimension\n","        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n","        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Compute the feed forward layer.\n","        \"\"\"\n","        return self.linear_2(torch.relu(self.linear_1(x)))"]},{"cell_type":"markdown","metadata":{},"source":["Decoder layers and decoder layer stacks are coded:"]},{"cell_type":"code","execution_count":235,"metadata":{},"outputs":[],"source":["class DecoderLayer(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for a decoder layer.\n","\n","    A decoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n","\n","    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n","    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            embedding_dimension,\n","            number_of_heads,\n","            feed_forward_dimension,\n","            dropout_rate\n","    ):\n","        super().__init__()\n","        self.embedding_dimension = embedding_dimension\n","        self.number_of_heads = number_of_heads\n","        self.feed_forward_dimension = feed_forward_dimension\n","        self.dropout_rate = dropout_rate\n","\n","        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n","        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n","        self.dropout = torch.nn.Dropout(dropout_rate)\n","        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n","        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n","\n","    def forward(self, x, mask):\n","        \"\"\"\n","        Compute the decoder layer.\n","\n","        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        mask dimensions are: (batch_size, sequence_length)\n","        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n","        \"\"\"\n","\n","        # Layer normalization 1\n","        normalized_x = self.layer_normalization_1(x)\n","\n","        # Multi headed self attention\n","        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n","\n","        # Residual output\n","        residual_output = x + attention_output\n","\n","        # Layer normalization 2\n","        normalized_residual_output = self.layer_normalization_2(residual_output)\n","\n","        # Feed forward\n","        feed_forward_output = self.feed_forward(normalized_residual_output)\n","\n","        # Dropout, only when training.\n","        if self.training:\n","            feed_forward_output = self.dropout(feed_forward_output)\n","\n","        # Residual output\n","        return residual_output + feed_forward_output"]},{"cell_type":"code","execution_count":236,"metadata":{},"outputs":[],"source":["class DecoderStack(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for a stack of decoders.\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            embedding_dimension,\n","            number_of_layers,\n","            number_of_heads,\n","            feed_forward_dimension,\n","            dropout_rate,\n","            max_sequence_length\n","    ):\n","        super().__init__()\n","        self.embedding_dimension = embedding_dimension\n","        self.number_of_layers = number_of_layers\n","        self.number_of_heads = number_of_heads\n","        self.feed_forward_dimension = feed_forward_dimension\n","        self.dropout_rate = dropout_rate\n","        self.max_sequence_length = max_sequence_length\n","\n","        # Create the encoder layers\n","        self.decoder_layers = torch.nn.ModuleList(\n","            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n","             range(number_of_layers)])\n","\n","    def forward(self, x, mask):\n","        decoder_outputs = x\n","        for decoder_layer in self.decoder_layers:\n","            decoder_outputs = decoder_layer(decoder_outputs, mask)\n","\n","        return decoder_outputs"]},{"cell_type":"markdown","metadata":{},"source":["The language model head:"]},{"cell_type":"code","execution_count":237,"metadata":{},"outputs":[],"source":["class LMHead(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for the language model head.\n","    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dimension, number_of_tokens):\n","        super().__init__()\n","        self.embedding_dimension = embedding_dimension\n","        self.number_of_tokens = number_of_tokens\n","        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Compute the language model head.\n","\n","        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n","        \"\"\"\n","        # Compute the linear layer\n","        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n","        linear_output = self.linear(x)\n","\n","        return linear_output"]},{"cell_type":"markdown","metadata":{},"source":["### The model."]},{"cell_type":"markdown","metadata":{},"source":["A class that defines the language model:"]},{"cell_type":"code","execution_count":273,"metadata":{},"outputs":[],"source":["import os\n","\n","class LanguageModel(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module for a language model.\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            number_of_tokens,  # The number of tokens in the vocabulary\n","            max_sequence_length=512,  # The maximum sequence length to use for attention\n","            embedding_dimension=512,  # The dimension of the token embeddings\n","            number_of_layers=6,  # The number of decoder layers to use\n","            number_of_heads=4,  # The number of attention heads to use\n","            feed_forward_dimension=None,  # The dimension of the feed forward layer\n","            dropout_rate=0.1  # The dropout rate to use\n","    ):\n","        super().__init__()\n","        self.number_of_tokens = number_of_tokens\n","        self.max_sequence_length = max_sequence_length\n","        self.embedding_dimension = embedding_dimension\n","        self.number_of_layers = number_of_layers\n","        self.number_of_heads = number_of_heads\n","\n","        if feed_forward_dimension is None:\n","            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n","            self.feed_forward_dimension = embedding_dimension * 4\n","        else:\n","            self.feed_forward_dimension = feed_forward_dimension\n","\n","        self.dropout_rate = dropout_rate\n","\n","        # Create the token embedding layer\n","        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n","\n","        # Create the positional encoding layer\n","        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n","\n","        # Create the normalization layer\n","        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n","\n","        # Create the decoder stack\n","        self.decoder = DecoderStack(\n","            embedding_dimension=embedding_dimension,\n","            number_of_layers=number_of_layers,\n","            number_of_heads=number_of_heads,\n","            feed_forward_dimension=self.feed_forward_dimension,\n","            dropout_rate=dropout_rate,\n","            max_sequence_length=max_sequence_length\n","        )\n","\n","        # Create the language model head\n","        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n","\n","    def forward(self, x, mask):\n","        # Compute the token embeddings\n","        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        token_embeddings = self.token_embedding(x)\n","\n","        # Compute the positional encoding\n","        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n","        positional_encoding = self.positional_encoding(token_embeddings)\n","\n","        # Post embedding layer normalization\n","        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n","\n","        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n","        lm_head_outputs = self.lm_head(decoder_outputs)\n","\n","        return lm_head_outputs\n","    \n","    def save_checkpoint(self, path, model_name, current_loss):\n","        print(f'Saving checkpoint {path}')\n","\n","        if not os.path.exists('models/' + model_name):\n","            os.mkdir('models/' + model_name)\n","\n","        torch.save({\n","            'number_of_tokens': self.number_of_tokens,\n","            'max_sequence_length': self.max_sequence_length,\n","            'embedding_dimension': self.embedding_dimension,\n","            'number_of_layers': self.number_of_layers,\n","            'number_of_heads': self.number_of_heads,\n","            'feed_forward_dimension': self.feed_forward_dimension,\n","            'dropout_rate': self.dropout_rate,\n","            'model_state_dict': self.state_dict()\n","        }, path)\n","\n","        with open('models/' + model_name + '/' + model_name + '_loss.txt', 'w') as file:\n","            file.write(str(current_loss))\n","\n","\n","    @staticmethod\n","    def load_checkpoint(path) -> 'LanguageModel':\n","        checkpoint = torch.load(path)\n","        model = LanguageModel(\n","            number_of_tokens=checkpoint['number_of_tokens'],\n","            max_sequence_length=checkpoint['max_sequence_length'],\n","            embedding_dimension=checkpoint['embedding_dimension'],\n","            number_of_layers=checkpoint['number_of_layers'],\n","            number_of_heads=checkpoint['number_of_heads'],\n","            feed_forward_dimension=checkpoint['feed_forward_dimension'],\n","            dropout_rate=checkpoint['dropout_rate']\n","        )\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        return model"]},{"cell_type":"markdown","metadata":{},"source":["Coding an autoregressive wrapper:"]},{"cell_type":"code","execution_count":274,"metadata":{},"outputs":[],"source":["class AutoregressiveWrapper(torch.nn.Module):\n","    \"\"\"\n","    Pytorch module that wraps a GPT model and makes it autoregressive.\n","    \"\"\"\n","\n","    def __init__(self, gpt_model):\n","        super().__init__()\n","        self.model = gpt_model\n","        self.max_sequence_length = self.model.max_sequence_length\n","\n","    def forward(self, x, mask):\n","        \"\"\"\n","        Autoregressive forward pass\n","        \"\"\"\n","        inp, target = x[:, :-1], x[:, 1:]\n","        mask = mask[:, :-1]\n","\n","        output = self.model(inp, mask)\n","        return output, target\n","\n","    def next_token_probabilities(self, x, mask, temperature=1.0):\n","        \"\"\"\n","        Calculate the token probabilities for the next token in the sequence.\n","        \"\"\"\n","        logits = self.model(x, mask)[:, -1]\n","\n","        # Apply the temperature\n","        if temperature != 1.0:\n","            logits = logits / temperature\n","\n","        # Apply the softmax\n","        probabilities = torch.softmax(logits, dim=-1)\n","\n","        return probabilities\n","    \n","    def save_checkpoint(self, path, model_name, current_loss):\n","        self.model.save_checkpoint(path, model_name, current_loss)\n","\n","    @staticmethod\n","    def load_checkpoint(path) -> 'AutoregressiveWrapper':\n","        model = LanguageModel.load_checkpoint(path)\n","        return AutoregressiveWrapper(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Training the model."]},{"cell_type":"markdown","metadata":{},"source":["The training pipeline is coded as follows:"]},{"cell_type":"code","execution_count":275,"metadata":{},"outputs":[],"source":["import random\n","\n","class Trainer:\n","\n","    def __init__(self, model, optimizer=None):\n","        super().__init__()\n","        self.model = model\n","        if optimizer is None:\n","            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","        else:\n","            self.optimizer = optimizer\n","        self.loss_function = torch.nn.CrossEntropyLoss()\n","\n","    def train(self, data: list[str], epochs, batch_size, model_name):\n","        loss_per_epoch = []\n","        for epoch in range(epochs):\n","            losses = []\n","\n","            # Shuffle the sequences\n","            random.shuffle(data)\n","\n","            # Create batches of sequences and their respective mask.\n","            batches = []\n","            for i in range(0, len(data), batch_size):\n","                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n","\n","                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n","                mask_tensor = torch.ones_like(sequence_tensor)\n","                mask_tensor[sequence_tensor == 0] = 0\n","\n","                batches.append((sequence_tensor, mask_tensor))\n","\n","            # Train the model on each batch\n","            for batch in batches:\n","                self.model.train()\n","\n","                # Create the input and mask tensors\n","                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n","                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n","\n","                for i, input_entry in enumerate(batch[0]):\n","                    input_tensor[i] = input_entry\n","\n","                for i, mask_entry in enumerate(batch[1]):\n","                    mask_tensor[i] = mask_entry\n","\n","                # Compute the model output\n","                model_output, target = self.model.forward(x=input_tensor, mask=mask_tensor)\n","\n","                # Compute the losses\n","                # The loss is computed on the model output and the target\n","                loss = self.loss_function(model_output.transpose(1, 2), target)\n","\n","                # Backpropagate the loss.\n","                loss.backward()\n","\n","                # Clip the gradients. This is used to prevent exploding gradients.\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n","\n","                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n","                self.optimizer.step()\n","\n","                # Reset the gradients. This is done so that the gradients from the previous batch\n","                # are not used in the next step.\n","                self.optimizer.zero_grad()\n","\n","                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n","                losses.append(loss.item())\n","\n","            # Print the loss\n","            epoch_loss = np.average(losses)\n","            loss_per_epoch.append(epoch_loss)\n","            print('Epoch:', epoch + 1, 'Loss:', epoch_loss)\n","\n","            if (epoch + 1) % 5 == 0:\n","                self.model.save_checkpoint('models/' + model_name + '/' + model_name, model_name, epoch_loss)\n","\n","        return loss_per_epoch"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 Loss: 4.410200036266475\n","Epoch: 2 Loss: 1.6455686115524144\n","Epoch: 3 Loss: 0.5773618327037802\n","Epoch: 4 Loss: 0.2545467774844864\n","Epoch: 5 Loss: 0.15870444928702798\n","Saving checkpoint models/model_1/model_1\n","Epoch: 6 Loss: 0.112640475203256\n","Epoch: 7 Loss: 0.08426733445556997\n","Epoch: 8 Loss: 0.0644065858778155\n","Epoch: 9 Loss: 0.05068288335251143\n","Epoch: 10 Loss: 0.0429190559375995\n","Saving checkpoint models/model_1/model_1\n","Epoch: 11 Loss: 0.036424185693245924\n","Epoch: 12 Loss: 0.030711722137288924\n","Epoch: 13 Loss: 0.025146619651864453\n","Epoch: 14 Loss: 0.02290428656218935\n","Epoch: 15 Loss: 0.01932277317527338\n","Saving checkpoint models/model_1/model_1\n","Epoch: 16 Loss: 0.01725852872150073\n","Epoch: 17 Loss: 0.016374266796261614\n","Epoch: 18 Loss: 0.015242967931598931\n","Epoch: 19 Loss: 0.012955165017188247\n","Epoch: 20 Loss: 0.012672155498508907\n","Saving checkpoint models/model_1/model_1\n","Epoch: 21 Loss: 0.01184241303651892\n","Epoch: 22 Loss: 0.010927177570168001\n","Epoch: 23 Loss: 0.009491385779602372\n","Epoch: 24 Loss: 0.010913085725649545\n","Epoch: 25 Loss: 0.009288518375873078\n","Saving checkpoint models/model_1/model_1\n","Epoch: 26 Loss: 0.006755745227169471\n","Epoch: 27 Loss: 0.008532364555556572\n","Epoch: 28 Loss: 0.006833496178812302\n","Epoch: 29 Loss: 0.006570401525427813\n","Epoch: 30 Loss: 0.007623300800926178\n","Saving checkpoint models/model_1/model_1\n","Epoch: 31 Loss: 0.007825890081302824\n","Epoch: 32 Loss: 0.005669126403772326\n","Epoch: 33 Loss: 0.007317243405059432\n","Epoch: 34 Loss: 0.005874814983563005\n","Epoch: 35 Loss: 0.008435383624226552\n","Saving checkpoint models/model_1/model_1\n","Epoch: 36 Loss: 0.004580763839490404\n","Epoch: 37 Loss: 0.005536048312700304\n","Epoch: 38 Loss: 0.006040955055928879\n","Epoch: 39 Loss: 0.006423701077204614\n","Epoch: 40 Loss: 0.005044008645743869\n","Saving checkpoint models/model_1/model_1\n","Epoch: 41 Loss: 0.005259166104570416\n","Epoch: 42 Loss: 0.005597046567066358\n","Epoch: 43 Loss: 0.0058885250074559485\n","Epoch: 44 Loss: 0.005086632904680547\n","Epoch: 45 Loss: 0.003939306253017191\n","Saving checkpoint models/model_1/model_1\n","Epoch: 46 Loss: 0.004069413013673767\n","Epoch: 47 Loss: 0.004716789609114289\n","Epoch: 48 Loss: 0.0037749804272466155\n","Epoch: 49 Loss: 0.005466771821198404\n","Epoch: 50 Loss: 0.003871784473788298\n","Saving checkpoint models/model_1/model_1\n","Epoch: 51 Loss: 0.00318192332171783\n","Epoch: 52 Loss: 0.003115356292831175\n","Epoch: 53 Loss: 0.006158270233791678\n","Epoch: 54 Loss: 0.0026465941593450076\n","Epoch: 55 Loss: 0.005235453149121824\n","Saving checkpoint models/model_1/model_1\n","Epoch: 56 Loss: 0.0042355233751612065\n","Epoch: 57 Loss: 0.005082884876152729\n","Epoch: 58 Loss: 0.0031828751492831695\n","Epoch: 59 Loss: 0.004405280057341143\n","Epoch: 60 Loss: 0.0029183997640536545\n","Saving checkpoint models/model_1/model_1\n","Epoch: 61 Loss: 0.003182190953747107\n","Epoch: 62 Loss: 0.004712470018272955\n","Epoch: 63 Loss: 0.003487250542718701\n","Epoch: 64 Loss: 0.002236189968649889\n","Epoch: 65 Loss: 0.003042548322631668\n","Saving checkpoint models/model_1/model_1\n","Epoch: 66 Loss: 0.005642447637011765\n","Epoch: 67 Loss: 0.00489798347086779\n","Epoch: 68 Loss: 0.0026927871427776234\n","Epoch: 69 Loss: 0.003000466804446351\n","Epoch: 70 Loss: 0.002502218682480127\n","Saving checkpoint models/model_1/model_1\n","Epoch: 71 Loss: 0.0030744900960077925\n","Epoch: 72 Loss: 0.002679032092775954\n","Epoch: 73 Loss: 0.0035016962031221236\n","Epoch: 74 Loss: 0.003937228883179928\n","Epoch: 75 Loss: 0.002149846562548474\n","Saving checkpoint models/model_1/model_1\n","Epoch: 76 Loss: 0.002935639680319441\n","Epoch: 77 Loss: 0.0018625118988054665\n","Epoch: 78 Loss: 0.0019402064477246526\n","Epoch: 79 Loss: 0.0033143126583400745\n","Epoch: 80 Loss: 0.003576532647791273\n","Saving checkpoint models/model_1/model_1\n","Epoch: 81 Loss: 0.004164235645979168\n","Epoch: 82 Loss: 0.0016546253081296263\n","Epoch: 83 Loss: 0.0042432933970436015\n","Epoch: 84 Loss: 0.005048575026683342\n","Epoch: 85 Loss: 0.002991845728961247\n","Saving checkpoint models/model_1/model_1\n","Epoch: 86 Loss: 0.0025273258934036666\n","Epoch: 87 Loss: 0.0031243968475553508\n","Epoch: 88 Loss: 0.0019124861601615088\n","Epoch: 89 Loss: 0.0017073924498565017\n","Epoch: 90 Loss: 0.0032400764811586448\n","Saving checkpoint models/model_1/model_1\n","Epoch: 91 Loss: 0.0034054882814248716\n","Epoch: 92 Loss: 0.0037934098109633726\n","Epoch: 93 Loss: 0.0022629177208109822\n","Epoch: 94 Loss: 0.001550800084878221\n","Epoch: 95 Loss: 0.002138464985417388\n","Saving checkpoint models/model_1/model_1\n","Epoch: 96 Loss: 0.0024359098618183406\n","Epoch: 97 Loss: 0.0027856650616175684\n","Epoch: 98 Loss: 0.003723251489063135\n","Epoch: 99 Loss: 0.0018428296048370206\n","Epoch: 100 Loss: 0.0018506328327683292\n","Saving checkpoint models/model_1/model_1\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","def create_training_sequences(max_sequence_length, tokenized_training_data):\n","    # Create sequences of length max_sequence_length + 1\n","    # The last token of each sequence is the target token\n","    sequences = []\n","    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n","        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n","    return sequences\n","\n","\n","def pad_training_data(max_sequence_length, tokenized_training_data):\n","    for _ in range(max_sequence_length):\n","        # Prepend padding tokens\n","        tokenized_training_data.insert(0, 0)\n","    return tokenized_training_data\n","\n","embedding_dimension = 256\n","max_sequence_length = 20\n","number_of_tokens = len(eval(tokenizer.get_config()['word_index']))\n","\n","# Create the model\n","model = AutoregressiveWrapper(LanguageModel(\n","    embedding_dimension=embedding_dimension,\n","    number_of_tokens=number_of_tokens,\n","    number_of_heads=4,\n","    number_of_layers=3,\n","    dropout_rate=0.1,\n","    max_sequence_length=max_sequence_length\n","))\n","\n","\n","tokenized_and_padded_training_data = pad_training_data(max_sequence_length, token_list)\n","sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n","\n","# Train the model\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","trainer = Trainer(model, optimizer)\n","loss_per_epoch = trainer.train(sequences, epochs=100, batch_size=8, model_name='model_1')\n","\n","# Plot the loss per epoch in log scale\n","plt.plot(loss_per_epoch)\n","plt.yscale('log')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing and saving the model."]},{"cell_type":"markdown","metadata":{},"source":["Saving the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save_checkpoint('models/model_1/model_1', 'model_1', loss_per_epoch[-1])"]},{"cell_type":"markdown","metadata":{},"source":["A generator class is coded to switch the model to evaluation mode. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def pad_left(sequence, final_length, padding_token):\n","    return [padding_token] * (final_length - len(sequence)) + sequence\n","\n","\n","class Generator:\n","\n","    def __init__(\n","            self,\n","            model,\n","            tokenizer):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.tokenizer_word_index = self.tokenizer.word_index\n","        self.tokenizer_index_word = self.tokenizer.index_word\n","        \n","\n","    def generate(\n","            self,\n","            max_tokens_to_generate: int,\n","            prompt: str = None,\n","            temperature: float = 1.0,\n","            eos_token: int = None,\n","            padding_token: int = 0):\n","\n","        self.model.eval()\n","\n","        self.tokenizer_word_index['<pad>'] = padding_token\n","        self.tokenizer_index_word[padding_token] = '<pad>'\n","\n","        if prompt is None:\n","            start_tokens = [0]\n","        else:\n","            self.tokenizer.fit_on_texts(prompt)\n","            start_tokens = text_to_token_list(self.tokenizer_word_index, prompt.lower())\n","            print(start_tokens)\n","\n","        input_tensor = torch.tensor(\n","            pad_left(\n","                sequence=start_tokens,\n","                final_length=self.model.max_sequence_length + 1,\n","                padding_token=padding_token\n","            ),\n","            dtype=torch.long\n","        )\n","\n","        num_dims = len(input_tensor.shape)\n","\n","        if num_dims == 1:\n","            input_tensor = input_tensor[None, :]\n","\n","        out = input_tensor\n","        for _ in range(max_tokens_to_generate):\n","\n","            x = out[:, -self.model.max_sequence_length:]\n","\n","            mask = torch.ones_like(x)\n","            mask[x == padding_token] = 0\n","\n","            # Compute the next token probabilities\n","            next_token_probabilities = self.model.next_token_probabilities(\n","                x=x,\n","                temperature=temperature,\n","                mask=mask\n","            )\n","\n","            # Sample the next token from the probability distribution\n","            next_token = torch.multinomial(next_token_probabilities, num_samples=1)\n","\n","            # Append the next token to the output\n","            out = torch.cat([out, next_token], dim=1)\n","\n","            # If the end of sequence token is reached, stop generating tokens\n","            if eos_token is not None and next_token == eos_token:\n","                break\n","\n","        generated_tokens = out[0].tolist()\n","        return ' '.join([self.tokenizer_index_word[token] for token in generated_tokens])"]},{"cell_type":"markdown","metadata":{},"source":["Testing the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1321, 8]\n"]},{"ename":"IndexError","evalue":"index out of range in self","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[196], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m max_tokens_to_generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      2\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator(model, tokenizer)\n\u001b[1;32m----> 3\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens_to_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens_to_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcity and\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","Cell \u001b[1;32mIn[183], line 60\u001b[0m, in \u001b[0;36mGenerator.generate\u001b[1;34m(self, max_tokens_to_generate, prompt, temperature, eos_token, padding_token)\u001b[0m\n\u001b[0;32m     57\u001b[0m mask[x \u001b[38;5;241m==\u001b[39m padding_token] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Compute the next token probabilities\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m next_token_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_token_probabilities\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Sample the next token from the probability distribution\u001b[39;00m\n\u001b[0;32m     67\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(next_token_probabilities, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[1;32mIn[114], line 25\u001b[0m, in \u001b[0;36mAutoregressiveWrapper.next_token_probabilities\u001b[1;34m(self, x, mask, temperature)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_token_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Calculate the token probabilities for the next token in the sequence.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Apply the temperature\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[103], line 56\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Compute the token embeddings\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Compute the positional encoding\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     positional_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(token_embeddings)\n","File \u001b[1;32mc:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[79], line 19\u001b[0m, in \u001b[0;36mTokenEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mIndexError\u001b[0m: index out of range in self"]}],"source":["max_tokens_to_generate = 50\n","generator = Generator(model, tokenizer)\n","generated_text = generator.generate(\n","    max_tokens_to_generate=max_tokens_to_generate,\n","    prompt=\"city and\",\n","    padding_token=0\n",")\n","print(generated_text.replace('<pad>', ''))"]},{"cell_type":"markdown","metadata":{},"source":["Loading the model:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = 'MODEL NAME HERE'\n","\n","language_model = LanguageModel.load_checkpoint(model_name)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":2}
